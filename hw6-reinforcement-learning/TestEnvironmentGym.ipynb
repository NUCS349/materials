{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This Jupyter notebook should help you a little bit with setting up HW6.\n",
    "<br>\n",
    "First we load an example Environment \"FrozenLake-v0\" (see https://reinforcementlearning4.fun/2019/06/16/gym-tutorial-frozen-lake/ ) and we explore how openAI works. Feel free to write you own little adaption of the code to become more familair with this.\n",
    "Afterwards I have copied the pytests for Multi-Armed-Bandit (MBA) and Q-Learning into this jupyter notebook and you can run those tests within this jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# frozen-lake-ex1.py\n",
    "import gym # loading the Gym library        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# When developping modules in python you want to automatically reload them when changed.\n",
    "# Please have a look here how this works:\n",
    "# https://switowski.com/blog/ipython-autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Action Space:\n",
      "Discrete(4)\n",
      "What is size of the Action Space:\n",
      "4\n",
      "Draw a few random samples: \n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()    \n",
    "\n",
    "print(\"What is the Action Space:\")\n",
    "print(env.action_space)\n",
    "print(\"What is size of the Action Space:\")\n",
    "print(env.action_space.n)\n",
    "\n",
    "print(\"Draw a few random samples: \")\n",
    "for k in range(0,5):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the observation space?\n",
      "Discrete(16)\n",
      "0\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(\"What is the observation space?\")\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.sample())\n",
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "env.render()\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(\n",
    "       random_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an environment that is not slippery to see the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- winning sequence ------ \n",
      "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.00\n",
      "{'prob': 1.0}\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.00\n",
      "{'prob': 1.0}\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.00\n",
      "{'prob': 1.0}\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Reward: 0.00\n",
      "{'prob': 1.0}\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Reward: 0.00\n",
      "{'prob': 1.0}\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Reward: 1.00\n",
      "{'prob': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actions = {\n",
    "    'Left': 0,\n",
    "    'Down': 1,\n",
    "    'Right': 2, \n",
    "    'Up': 3\n",
    "}\n",
    " \n",
    "print('---- winning sequence ------ ')\n",
    "winning_sequence = (2 * ['Right']) + (3 * ['Down']) + ['Right']\n",
    "print(winning_sequence)\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\",is_slippery=False) \n",
    "# TASK: set is_slippery to false and observe what happens\n",
    "env.reset()\n",
    "env.render()\n",
    " \n",
    "for a in winning_sequence:\n",
    "    new_state, reward, done, info = env.step(actions[a])\n",
    "    print()\n",
    "    env.render()\n",
    "    print(\"Reward: {:.2f}\".format(reward))\n",
    "    print(info)\n",
    "    if done:\n",
    "        break  \n",
    " \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement MultiArmedBandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Step: Go to src \"MultiArmedBandit\" and make the changes in code, then come back here and see the effects of the changes live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import MultiArmedBandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For passing the FrozenLake we only have to implement the .fit functions. So it might be a good idea to start with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]\n",
      " [0.         0.00655022 0.00654854 0.00512821]]\n",
      "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 0.   0.02 0.02 0.01\n",
      " 0.   0.01 0.01 0.03 0.   0.01 0.   0.01 0.01 0.02 0.   0.   0.01 0.\n",
      " 0.   0.   0.   0.   0.02 0.02 0.   0.02 0.01 0.01 0.01 0.01 0.   0.01\n",
      " 0.   0.   0.   0.01 0.   0.   0.   0.02 0.01 0.   0.   0.   0.02 0.\n",
      " 0.   0.   0.   0.01 0.   0.01 0.   0.02 0.   0.01 0.01 0.01 0.   0.\n",
      " 0.   0.02 0.   0.01 0.02 0.01 0.02 0.01 0.02 0.01 0.01 0.   0.   0.\n",
      " 0.01 0.   0.   0.   0.01 0.01 0.02 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.seed(0)\n",
    "\n",
    "agent = MultiArmedBandit(epsilon=0.2)\n",
    "state_action_values, rewards = agent.fit(env, steps=10000)\n",
    "\n",
    "assert state_action_values.shape == (16, 4)\n",
    "assert len(rewards) == 100\n",
    "\n",
    "print(state_action_values)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we implement the predict function, for that we use a different gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('SlotMachines-v0', n_machines=10, mean_range=(-10, 10), std_range=(5, 10))\n",
    "means = np.array([m.mean for m in env.machines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97627008  4.30378733  2.05526752  0.89766366 -1.52690401  2.91788226\n",
      " -1.24825577  7.83546002  9.27325521 -2.33116962]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8a284bb1f0>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(means)\n",
    "plt.plot(means,'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MultiArmedBandit(epsilon=0.2)\n",
    "state_action_values, rewards = agent.fit(env, steps=10000)\n",
    "\n",
    "assert state_action_values.shape == (1, 10)\n",
    "assert len(rewards) == 100\n",
    "assert np.argmax(means) == np.argmax(state_action_values)\n",
    "\n",
    "states, actions, rewards_predict = agent.predict(env, state_action_values)\n",
    "assert len(states) == 1\n",
    "assert len(actions) == 1 and actions[0] == np.argmax(means)\n",
    "assert len(rewards_predict) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the desterimnistic environment for Q-learning\n",
    "This requires implementation of the \"predict\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tests that the QLearning implementation successfully navigates a\n",
    "deterministic environment with provided state-action-values.\n",
    "\"\"\"\n",
    "from src import QLearning\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozonLakeNoSlippery-v0')\n",
    "env.reset()\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearning(epsilon=0.5, discount=0.95)\n",
    "state_action_values = np.array([\n",
    "    [0.0, 0.7, 0.3, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0, 0.0],\n",
    "    [0.0, 0.51, 0.49, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.2, 0.8, 0.0],\n",
    "    [0.0, 0.2, 0.8, 0.0],\n",
    "    [0.0, 0.6, 0.4, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "states, actions, rewards = agent.predict(env, state_action_values)\n",
    "assert np.all(states == np.array([4, 8, 9, 10, 14, 15]))\n",
    "assert np.all(actions == np.array([1, 1, 2, 2, 1, 2]))\n",
    "assert np.all(rewards == np.array([0, 0, 0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now implement the FIT function for Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tests that the QLearning implementation successfully learns the\n",
    "FrozenLake-v0 environment.\n",
    "\"\"\"\n",
    "from src import QLearning\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Action Value (shape) (16, 4)\n",
      "Action count (shape): (16, 4)\n"
     ]
    }
   ],
   "source": [
    "agent = QLearning(epsilon=0.2, discount=0.95)\n",
    "state_action_values, rewards = agent.fit(env, steps=10000)\n",
    "\n",
    "state_values = np.max(state_action_values, axis=1)\n",
    "\n",
    "assert state_action_values.shape == (16, 4)\n",
    "assert len(rewards) == 100\n",
    "\n",
    "assert np.allclose(state_values[np.array([5, 7, 11, 12, 15])], np.zeros(5))\n",
    "assert np.all(state_values[np.array([0, 1, 2, 3, 4, 6, 8, 9, 10, 13, 14])] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00960789 0.01228849 0.01085123 0.00614481]\n",
      " [0.00569692 0.00808829 0.0110898  0.01645868]\n",
      " [0.03112818 0.02270186 0.02496603 0.01437928]\n",
      " [0.00902774 0.0060459  0.00462023 0.01786018]\n",
      " [0.02798165 0.03052711 0.01283507 0.0078561 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06509822 0.02555111 0.05716095 0.00914774]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02329913 0.06674491 0.03750659 0.07653262]\n",
      " [0.0699447  0.16974907 0.14452346 0.07438071]\n",
      " [0.19484704 0.15225591 0.14858419 0.06399919]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09386677 0.16338306 0.27823305 0.16990609]\n",
      " [0.25290769 0.32686855 0.30253787 0.45836909]\n",
      " [0.         0.         0.         0.        ]]\n",
      "(100,)\n",
      "[0.   0.   0.01 0.02 0.01 0.   0.   0.   0.01 0.   0.01 0.   0.01 0.\n",
      " 0.01 0.   0.   0.   0.   0.01 0.   0.   0.01 0.03 0.   0.   0.   0.\n",
      " 0.   0.01 0.01 0.01 0.01 0.   0.02 0.02 0.   0.   0.01 0.   0.   0.01\n",
      " 0.   0.01 0.01 0.   0.   0.   0.   0.01 0.   0.02 0.   0.   0.   0.\n",
      " 0.03 0.   0.01 0.   0.02 0.01 0.   0.01 0.   0.01 0.01 0.02 0.   0.03\n",
      " 0.   0.01 0.   0.   0.01 0.01 0.02 0.01 0.02 0.   0.01 0.   0.   0.\n",
      " 0.   0.01 0.01 0.01 0.01 0.   0.   0.   0.01 0.   0.02 0.   0.   0.\n",
      " 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(state_action_values)\n",
    "print(rewards.shape)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try out if it works for 'SlotMachines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Action Value (shape) (1, 10)\n",
      "Action count (shape): (1, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tests that the Qlearning implementation successfully finds the slot\n",
    "machine with the largest expected reward.\n",
    "\"\"\"\n",
    "from src import QLearning\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('SlotMachines-v0', n_machines=10, mean_range=(-10, 10), std_range=(5, 10))\n",
    "env.seed(0)\n",
    "means = np.array([m.mean for m in env.machines])\n",
    "\n",
    "agent = QLearning(epsilon=0.2, discount=0)\n",
    "state_action_values, rewards = agent.fit(env, steps=10000)\n",
    "\n",
    "assert state_action_values.shape == (1, 10)\n",
    "assert len(rewards) == 100\n",
    "assert np.argmax(means) == np.argmax(state_action_values)\n",
    "\n",
    "states, actions, rewards = agent.predict(env, state_action_values)\n",
    "assert len(actions) == 1 and actions[0] == np.argmax(means)\n",
    "assert len(states) == 1 and states[0] == 0\n",
    "assert len(rewards) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.9223177]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
