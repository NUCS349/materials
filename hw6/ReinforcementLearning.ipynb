{"cells":[{"metadata":{},"cell_type":"markdown","source":"# HW6 - Reinforcement Learning!"},{"metadata":{},"cell_type":"markdown","source":"Overall, the code for QLearning and Multi-Armed-Bandit is pretty similar. I think the psuedocode for QLearning is easier to understand, but there is more to implement. The pseudocode for Multi-Armed-Bandit is kind of hard to understand, but there is less to implement\n"},{"metadata":{},"cell_type":"markdown","source":"### MULTI ARMED BANDIT\n\nFor the fit function, the comments in the description of this function are pretty helpful. \nLooking at the pseudocode in the Sutton & Barto book, it is actually kind of confusing, but if you're able to break up each variable and understand in conjunction with the lecture well, it should make more sense. I think the lecture video (13) is more helpful with his slides about the pseudocode. \nQ represents the state_action_values and then R is the rewards. Q should be a 2D array based on the states and actions\nWhen you're deciding which action to choose, it could be helpful to write a helper function based on the epsilon/row, but not necessary. Just make sure as you are deciding what to do that the random number is new for each iteration\n\nyou can use the two random functions below to generate a random decimal between 0-1 or a random int based on the range inside the parenthesis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.random() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.randint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the predict function, this is simpler. Just make sure to keep track of if the next step is done with the env.step() function and be sure to return 3 arrays. You only need to reset the environment once and you can use that to decide the next actions"},{"metadata":{},"cell_type":"markdown","source":"### QLEARNING\nFor this you'll also be implementing fit and predict and the one function at the bottom called adaptive_epsilon\nIn fit, you can initialize the action, state, and values all first and the same as Multi-Armed Bandit above ^\n\nRemember as you are choosing a next step based on epsilon and the row of the values the different branch cases for returning the choice. In the pseudocode, the gamma symbol is self.discount, but you can follow the equation as it's written out there. S' stands for the next state that is decided off the current action. You can use np.max() for when the equation says to take the maximum for the values of the next step\n\nThe following functions may be useful in deciding the action to take. "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.uniform(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.randint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For predict you can write it exactly the same as I've described in multi-armed bandit above^\n\nFor adaptive epsilon, don't think too hard about this. It can be one line of code with a simple equation"},{"metadata":{},"cell_type":"markdown","source":"## Experiments\nFor the FRQs, you'll be making plots based off the learning models. You can just make new instances of the MultiArmedBandit or QLearning objects with the different parameters and be plotting the results of the rewards after fitting in a loop\nYou might have to use np.divide to better visualize the data in the graphs\n\nDon't be nervous if the code is a taking some time to run"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
